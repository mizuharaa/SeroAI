#!/usr/bin/env python3
"""
Efficient watermark detection script for AI-generated videos.
Detects watermarks from SoraAI, Veo, Runway, and other AI video generators.

Strategy:
- Sample frames at intervals (not every frame)
- Focus on common watermark locations (corners, bottom center)
- Use OCR for text watermarks
- Use pattern matching for logos/consistent overlays
- Check for persistent elements across frames
"""

import cv2
import numpy as np
import argparse
import os
from typing import Dict, List, Tuple, Optional
from pathlib import Path
import json

# Optional dependencies
try:
    import pytesseract
    HAS_TESSERACT = True
except ImportError:
    HAS_TESSERACT = False
    print("[WARNING] pytesseract not installed. Text watermark detection will be limited.")

try:
    from Levenshtein import distance as levenshtein_distance
    HAS_LEVENSHTEIN = True
except ImportError:
    HAS_LEVENSHTEIN = False
    print("[WARNING] python-Levenshtein not installed. Text matching will be less accurate.")


# Known watermark keywords and patterns (prioritize common ones)
WATERMARK_KEYWORDS = [
    # OpenAI Sora (most common)
    'sora', 'soraai', 'openai', 'open ai',
    # Google
    'veo', 'google',
    # Runway
    'runway', 'runwayml', 'runway ml',
    # Pika
    'pika', 'pikalabs', 'pika labs',
    # Kling
    'kling', 'klingai', 'kling ai',
    # Stability AI
    'stability', 'stable video', 'stability ai',
    # Meta
    'meta', 'make-a-video', 'make a video',
    # Gen-2
    'gen-2', 'gen2', 'gen 2',
    # Generic
    'ai generated', 'generated by', 'generated with',
]

# Watermark locations to check (normalized coordinates: [x, y, width, height])
# Expanded to include mid-upper, mid-lower, and side regions for moving watermarks
WATERMARK_REGIONS = {
    # Corners
    'top_left': (0.0, 0.0, 0.15, 0.15),
    'top_right': (0.85, 0.0, 0.15, 0.15),
    'bottom_left': (0.0, 0.85, 0.15, 0.15),
    'bottom_right': (0.85, 0.85, 0.15, 0.15),
    # Center regions
    'bottom_center': (0.40, 0.90, 0.20, 0.10),
    'center': (0.45, 0.45, 0.10, 0.10),
    # Mid-upper and mid-lower (for moving watermarks)
    'mid_upper_left': (0.0, 0.25, 0.20, 0.15),
    'mid_upper_center': (0.40, 0.25, 0.20, 0.15),
    'mid_upper_right': (0.80, 0.25, 0.20, 0.15),
    'mid_lower_left': (0.0, 0.60, 0.20, 0.15),
    'mid_lower_center': (0.40, 0.60, 0.20, 0.15),
    'mid_lower_right': (0.80, 0.60, 0.20, 0.15),
    # Side edges (for watermarks on vertical edges)
    'left_edge_top': (0.0, 0.0, 0.10, 0.30),
    'left_edge_mid': (0.0, 0.35, 0.10, 0.30),
    'left_edge_bottom': (0.0, 0.70, 0.10, 0.30),
    'right_edge_top': (0.90, 0.0, 0.10, 0.30),
    'right_edge_mid': (0.90, 0.35, 0.10, 0.30),
    'right_edge_bottom': (0.90, 0.70, 0.10, 0.30),
}


def extract_region(frame: np.ndarray, region: Tuple[float, float, float, float]) -> np.ndarray:
    """Extract a region from frame based on normalized coordinates."""
    h, w = frame.shape[:2]
    x, y, rw, rh = region
    x1, y1 = int(x * w), int(y * h)
    x2, y2 = int((x + rw) * w), int((y + rh) * h)
    return frame[y1:y2, x1:x2]


def detect_text_in_region(region: np.ndarray) -> List[str]:
    """Detect text in a region using OCR with improved preprocessing for small watermark text."""
    if not HAS_TESSERACT:
        return []
    
    try:
        # Preprocess for better OCR
        gray = cv2.cvtColor(region, cv2.COLOR_BGR2GRAY) if len(region.shape) == 3 else region
        
        # Resize if too small (watermarks are often small, need to upscale for OCR)
        h, w = gray.shape
        if h < 30 or w < 30:
            # Upscale small regions for better OCR
            scale_factor = max(30 / h, 30 / w, 2.0)  # At least 2x upscale
            new_h, new_w = int(h * scale_factor), int(w * scale_factor)
            gray = cv2.resize(gray, (new_w, new_h), interpolation=cv2.INTER_CUBIC)
        
        # Multiple preprocessing strategies for watermark text
        texts = []
        
        # Strategy 1: Strong contrast enhancement (for semi-transparent watermarks)
        enhanced = cv2.convertScaleAbs(gray, alpha=2.5, beta=50)  # Stronger enhancement
        text1 = pytesseract.image_to_string(
            enhanced,
            config='--psm 8 -c tessedit_char_whitelist=abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789- '  # PSM 8 for single word
        )
        texts.extend([t.strip().lower() for t in text1.split() if len(t.strip()) >= 2])
        
        # Strategy 2: Invert colors (for white text on dark background)
        inverted = cv2.bitwise_not(gray)
        enhanced_inv = cv2.convertScaleAbs(inverted, alpha=2.5, beta=50)
        text2 = pytesseract.image_to_string(
            enhanced_inv,
            config='--psm 8 -c tessedit_char_whitelist=abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789- '
        )
        texts.extend([t.strip().lower() for t in text2.split() if len(t.strip()) >= 2])
        
        # Strategy 3: OTSU thresholding
        _, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
        text3 = pytesseract.image_to_string(
            thresh,
            config='--psm 8 -c tessedit_char_whitelist=abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789- '
        )
        texts.extend([t.strip().lower() for t in text3.split() if len(t.strip()) >= 2])
        
        # Strategy 4: Adaptive threshold
        adaptive = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2)
        text4 = pytesseract.image_to_string(
            adaptive,
            config='--psm 8 -c tessedit_char_whitelist=abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789- '
        )
        texts.extend([t.strip().lower() for t in text4.split() if len(t.strip()) >= 2])
        
        # Strategy 5: Morphological operations to enhance text
        kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (2, 2))
        morph = cv2.morphologyEx(enhanced, cv2.MORPH_CLOSE, kernel)
        text5 = pytesseract.image_to_string(
            morph,
            config='--psm 8 -c tessedit_char_whitelist=abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789- '
        )
        texts.extend([t.strip().lower() for t in text5.split() if len(t.strip()) >= 2])
        
        return list(set(texts))  # Remove duplicates
    except Exception as e:
        return []


def is_likely_subtitle(texts: List[str], region_name: str) -> bool:
    """Check if detected text is likely a subtitle (not a watermark)."""
    if not texts:
        return False
    
    # Subtitles are usually:
    # 1. Multiple words (watermarks are usually 1-2 words max)
    all_text = ' '.join(texts).lower()
    word_count = len(all_text.split())
    if word_count > 5:  # More than 5 words = likely subtitle
        return True
    
    # 2. At bottom center (common subtitle location)
    if region_name == 'bottom_center' and word_count > 2:
        return True
    
    # 3. Contains common subtitle words (not watermark keywords)
    subtitle_indicators = ['the', 'and', 'or', 'but', 'with', 'that', 'this', 'was', 'were', 'have', 'has', 'had']
    if any(indicator in all_text for indicator in subtitle_indicators) and word_count > 3:
        return True
    
    return False


def check_keyword_match(texts: List[str], keywords: List[str]) -> Tuple[bool, str, float]:
    """Check if any detected text matches watermark keywords with improved matching."""
    if not texts:
        return False, "", 0.0
    
    best_match = ""
    best_confidence = 0.0
    
    for text in texts:
        text_lower = text.lower().strip()
        
        for keyword in keywords:
            keyword_lower = keyword.lower().strip()
            
            # Exact match (highest confidence)
            if text_lower == keyword_lower:
                return True, keyword, 1.0
            
            # Substring match (high confidence)
            if keyword_lower in text_lower or text_lower in keyword_lower:
                # Boost confidence for short keywords like "sora" (4 chars)
                if len(keyword_lower) <= 5:
                    confidence = 0.95
                else:
                    confidence = 0.90
                if confidence > best_confidence:
                    best_confidence = confidence
                    best_match = keyword
                continue
            
            if HAS_LEVENSHTEIN:
                # Fuzzy matching with adjusted threshold for short words
                dist = levenshtein_distance(text_lower, keyword_lower)
                max_len = max(len(text_lower), len(keyword_lower))
                
                if max_len == 0:
                    continue
                
                similarity = 1.0 - (dist / max_len)
                
                # Lower threshold for short keywords (like "sora" = 4 chars)
                threshold = 0.6 if len(keyword_lower) <= 5 else 0.7
                
                if similarity > threshold:
                    if similarity > best_confidence:
                        best_confidence = similarity
                        best_match = keyword
            else:
                # Character-based matching for short words
                if len(keyword_lower) <= 5:
                    # Check if most characters match
                    matching_chars = sum(1 for c in keyword_lower if c in text_lower)
                    if matching_chars >= len(keyword_lower) * 0.7:  # 70% of chars match
                        confidence = matching_chars / len(keyword_lower)
                        if confidence > best_confidence:
                            best_confidence = confidence
                            best_match = keyword
    
    return best_confidence > 0.0, best_match, best_confidence


def detect_persistent_pattern(frames: List[np.ndarray], region: Tuple[float, float, float, float]) -> float:
    """Detect if a pattern persists across frames (indicates watermark)."""
    if len(frames) < 3:
        return 0.0
    
    regions = [extract_region(frame, region) for frame in frames]
    
    # Convert to grayscale and normalize
    gray_regions = []
    for r in regions:
        if len(r.shape) == 3:
            gray = cv2.cvtColor(r, cv2.COLOR_BGR2GRAY)
        else:
            gray = r
        gray_regions.append(cv2.resize(gray, (64, 64)))  # Normalize size
    
    # Calculate structural similarity between consecutive frames
    similarities = []
    for i in range(len(gray_regions) - 1):
        # Use template matching or histogram comparison
        hist1 = cv2.calcHist([gray_regions[i]], [0], None, [256], [0, 256])
        hist2 = cv2.calcHist([gray_regions[i+1]], [0], None, [256], [0, 256])
        similarity = cv2.compareHist(hist1, hist2, cv2.HISTCMP_CORREL)
        similarities.append(similarity)
    
    # High similarity across frames = persistent watermark
    avg_similarity = float(np.mean(similarities)) if similarities else 0.0
    return avg_similarity


def has_watermark_characteristics(region: np.ndarray) -> Tuple[bool, float]:
    """
    Check if a region has watermark-like characteristics:
    - Small, distinct logo-like shape
    - High contrast
    - Not just a uniform background
    Returns: (is_watermark_like, confidence)
    """
    if region.size == 0:
        return False, 0.0
    
    gray = cv2.cvtColor(region, cv2.COLOR_BGR2GRAY) if len(region.shape) == 3 else region
    h, w = gray.shape
    
    # Watermarks are small - region should be < 10% of typical frame
    # If region is too large, it's likely background, not watermark
    region_area = h * w
    if region_area > 10000:  # Too large for a watermark
        return False, 0.0
    
    # Check for high contrast (watermarks have distinct edges)
    edges = cv2.Canny(gray, 50, 150)
    edge_density = np.sum(edges > 0) / (h * w)
    
    # Check for distinct shapes (not uniform)
    gray_array = np.asarray(gray)
    std_dev = float(np.std(gray_array))  # type: ignore[arg-type]
    
    # Check for logo-like patterns (high edge density + distinct regions)
    # Watermarks typically have edge_density > 0.1 and std_dev > 20
    is_watermark_like = edge_density > 0.08 and std_dev > 15
    
    # Confidence based on how strong the characteristics are
    confidence = min(1.0, (edge_density * 5.0) + (std_dev / 50.0))
    
    return is_watermark_like, confidence


def track_moving_watermark(frames: List[np.ndarray], initial_region: Tuple[float, float, float, float], search_margin: float = 0.1) -> Tuple[float, List[Tuple[float, float, float, float]]]:
    """
    Track a watermark that may move between frames.
    
    Args:
        frames: List of video frames
        initial_region: Initial region where watermark was detected (normalized coords)
        search_margin: How much to expand search area (as fraction of frame size)
    
    Returns:
        Tuple of (tracking_confidence, list of tracked regions)
    """
    if len(frames) < 2:
        return 0.0, []
    
    # Extract template from first frame
    template = extract_region(frames[0], initial_region)
    if template.size == 0:
        return 0.0, []
    
    template_gray = cv2.cvtColor(template, cv2.COLOR_BGR2GRAY) if len(template.shape) == 3 else template
    template_gray = cv2.resize(template_gray, (64, 64))
    
    tracked_regions = [initial_region]
    match_scores = [1.0]
    
    h, w = frames[0].shape[:2]
    search_w = int(w * search_margin)
    search_h = int(h * search_margin)
    
    # Track in subsequent frames
    for i in range(1, len(frames)):
        frame_gray = cv2.cvtColor(frames[i], cv2.COLOR_BGR2GRAY) if len(frames[i].shape) == 3 else frames[i]
        
        # Expand search area around previous location
        prev_x, prev_y, prev_w, prev_h = tracked_regions[-1]
        x1 = max(0, int((prev_x - search_margin) * w))
        y1 = max(0, int((prev_y - search_margin) * h))
        x2 = min(w, int((prev_x + prev_w + search_margin) * w))
        y2 = min(h, int((prev_y + prev_h + search_margin) * h))
        
        search_area = frame_gray[y1:y2, x1:x2]
        
        if search_area.size == 0 or search_area.shape[0] < template_gray.shape[0] or search_area.shape[1] < template_gray.shape[1]:
            tracked_regions.append(initial_region)  # Fallback
            match_scores.append(0.0)
            continue
        
        # Template matching
        result = cv2.matchTemplate(search_area, template_gray, cv2.TM_CCOEFF_NORMED)
        _, max_val, _, max_loc = cv2.minMaxLoc(result)
        
        # Convert back to normalized coordinates
        found_x = (x1 + max_loc[0]) / w
        found_y = (y1 + max_loc[1]) / h
        found_w = prev_w
        found_h = prev_h
        
        tracked_regions.append((found_x, found_y, found_w, found_h))
        match_scores.append(float(max_val))
    
    # Average match score indicates tracking confidence
    avg_match = float(np.mean(match_scores)) if match_scores else 0.0
    return avg_match, tracked_regions


def find_watermark_candidates(frame: np.ndarray) -> List[Tuple[float, float, float, float]]:
    """
    Find potential watermark regions using edge detection and contour analysis.
    Useful for detecting watermarks that don't match fixed regions.
    """
    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) if len(frame.shape) == 3 else frame
    h, w = gray.shape
    
    # Edge detection
    edges = cv2.Canny(gray, 50, 150)
    
    # Find contours
    contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    
    candidates = []
    for contour in contours:
        x, y, cw, ch = cv2.boundingRect(contour)
        
        # Filter by size (watermarks are typically small relative to frame)
        area = cw * ch
        frame_area = w * h
        if 0.001 * frame_area < area < 0.1 * frame_area:  # 0.1% to 10% of frame
            # Check aspect ratio (watermarks are often wider than tall)
            aspect_ratio = cw / ch if ch > 0 else 0
            if 0.5 < aspect_ratio < 5.0:  # Reasonable aspect ratio
                # Normalize coordinates
                norm_x = x / w
                norm_y = y / h
                norm_w = cw / w
                norm_h = ch / h
                candidates.append((norm_x, norm_y, norm_w, norm_h))
    
    return candidates


def detect_logo_pattern(region: np.ndarray) -> float:
    """Detect logo-like patterns (high contrast, distinct shapes)."""
    if len(region.shape) == 3:
        gray = cv2.cvtColor(region, cv2.COLOR_BGR2GRAY)
    else:
        gray = region
    
    # Check for high contrast (typical of logos)
    edges = cv2.Canny(gray, 50, 150)
    edge_density = np.sum(edges > 0) / (edges.shape[0] * edges.shape[1])
    
    # Check for distinct color regions (logos often have solid colors)
    if len(region.shape) == 3:
        region_array = np.asarray(region)
        std_dev = float(np.std(region_array, axis=2).mean())  # type: ignore[arg-type]
    else:
        gray_array = np.asarray(gray)
        std_dev = float(np.std(gray_array))  # type: ignore[arg-type]
    
    # Combine metrics
    logo_score = (edge_density * 0.5) + (min(std_dev / 50.0, 1.0) * 0.5)
    return logo_score


def extract_watermark_image(frames: List[np.ndarray], region: Tuple[float, float, float, float], output_path: Optional[str] = None) -> np.ndarray:
    """Extract and average watermark region across frames to get a clear image."""
    regions = [extract_region(frame, region) for frame in frames]
    
    # Average the regions to get a clearer watermark image
    if regions:
        # Resize all to same size
        target_size = (regions[0].shape[1], regions[0].shape[0])
        resized = [cv2.resize(r, target_size) for r in regions]
        
        # Average - convert to numpy array first
        resized_array = np.asarray(resized)
        avg_region = np.mean(resized_array, axis=0).astype(np.uint8)  # type: ignore[arg-type]
        
        # Enhance contrast for better visibility
        if len(avg_region.shape) == 3:
            gray = cv2.cvtColor(avg_region, cv2.COLOR_BGR2GRAY)
        else:
            gray = avg_region
        
        # Multiple enhancement strategies
        # Strategy 1: High contrast enhancement
        enhanced = cv2.convertScaleAbs(gray, alpha=2.0, beta=50)
        
        # Strategy 2: Adaptive threshold (better for varying backgrounds)
        adaptive = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2)
        
        # Strategy 3: OTSU threshold
        _, otsu = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
        
        # Strategy 4: Invert (for white text on dark)
        inverted = cv2.bitwise_not(gray)
        _, inverted_thresh = cv2.threshold(inverted, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
        
        # Use the one with most edges (likely has text/logo)
        edge_counts = [
            (np.sum(cv2.Canny(enhanced, 50, 150) > 0), enhanced, 'enhanced'),
            (np.sum(cv2.Canny(adaptive, 50, 150) > 0), adaptive, 'adaptive'),
            (np.sum(cv2.Canny(otsu, 50, 150) > 0), otsu, 'otsu'),
            (np.sum(cv2.Canny(inverted_thresh, 50, 150) > 0), inverted_thresh, 'inverted'),
        ]
        edge_counts.sort(reverse=True, key=lambda x: x[0])
        best_image = edge_counts[0][1]
        
        if output_path:
            # Save the best version
            cv2.imwrite(output_path, best_image)
            # Also save enhanced version for debugging
            enhanced_path = output_path.replace('.png', '_enhanced.png')
            cv2.imwrite(enhanced_path, enhanced)
            print(f"[INFO] Watermark image saved to: {output_path} (best: {edge_counts[0][2]})")
        
        return best_image
    return np.array([])


def identify_logo_from_text(region: np.ndarray) -> Tuple[str, float]:
    """Try to identify logo by reading text in the region with multiple OCR strategies."""
    if not HAS_TESSERACT:
        return "", 0.0
    
    # Strategy 1: Standard text detection
    texts = detect_text_in_region(region)
    has_match, matched_keyword, confidence = check_keyword_match(texts, WATERMARK_KEYWORDS)
    
    if has_match:
        return matched_keyword, confidence
    
    # Strategy 2: Try OCR on the region directly with different PSM modes
    if region.size > 0:
        gray = cv2.cvtColor(region, cv2.COLOR_BGR2GRAY) if len(region.shape) == 3 else region
        
        # Try different PSM modes for better text detection
        psm_modes = [
            '--psm 7',  # Single text line
            '--psm 8',  # Single word
            '--psm 6',  # Single uniform block
            '--psm 11', # Sparse text
        ]
        
        all_detected_texts = []
        for psm in psm_modes:
            try:
                # Try with enhanced contrast
                enhanced = cv2.convertScaleAbs(gray, alpha=2.0, beta=50)
                text = pytesseract.image_to_string(enhanced, config=f'{psm} -c tessedit_char_whitelist=abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789- ')
                all_detected_texts.extend([t.strip().lower() for t in text.split() if len(t.strip()) > 1])
                
                # Try with inverted
                inverted = cv2.bitwise_not(gray)
                text2 = pytesseract.image_to_string(inverted, config=f'{psm} -c tessedit_char_whitelist=abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789- ')
                all_detected_texts.extend([t.strip().lower() for t in text2.split() if len(t.strip()) > 1])
            except:
                continue
        
        # Check matches from all detected texts
        if all_detected_texts:
            has_match2, matched_keyword2, confidence2 = check_keyword_match(list(set(all_detected_texts)), WATERMARK_KEYWORDS)
            if has_match2 and confidence2 > confidence:
                return matched_keyword2, confidence2
    
    # Strategy 3: Return any text found (might be partial match)
    if texts:
        # Check if any text is similar to keywords (even if not exact match)
        for text in texts:
            for keyword in WATERMARK_KEYWORDS:
                if len(text) >= 3 and len(keyword) >= 3:
                    # Simple character overlap check
                    common_chars = set(text) & set(keyword)
                    if len(common_chars) >= min(len(text), len(keyword)) * 0.6:  # 60% overlap
                        return keyword, 0.6  # Medium confidence
    
    return "", 0.0


def sample_frame_indices(total_frames: int, max_samples: int = 20) -> List[int]:
    """Sample frame indices evenly across the video."""
    if total_frames <= max_samples:
        return list(range(total_frames))
    
    # Sample evenly spaced frames
    indices = np.linspace(0, total_frames - 1, num=max_samples, dtype=int)
    # Remove duplicates and sort
    return sorted(list(set(indices.tolist())))


def extract_watermark_text_from_frame(frame: np.ndarray) -> List[Dict]:
    """
    Extract watermark text from a full frame using OCR.
    Returns list of detections with text and keyword match.
    """
    if not HAS_TESSERACT:
        return []
    
    detections = []
    
    try:
        # Convert to RGB for pytesseract
        if len(frame.shape) == 3:
            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        else:
            rgb_frame = frame
        
        # Run OCR on full frame with multiple preprocessing strategies
        all_texts = []
        
        # Strategy 1: Direct OCR on full frame
        try:
            text1 = pytesseract.image_to_string(rgb_frame, config='--psm 6')
            words1 = [w.strip().lower() for w in text1.split() if len(w.strip()) >= 2]
            all_texts.extend(words1)
        except:
            pass
        
        # Strategy 2: Enhanced contrast (for semi-transparent watermarks)
        try:
            gray = cv2.cvtColor(rgb_frame, cv2.COLOR_RGB2GRAY) if len(rgb_frame.shape) == 3 else rgb_frame
            enhanced = cv2.convertScaleAbs(gray, alpha=2.5, beta=50)
            text2 = pytesseract.image_to_string(enhanced, config='--psm 6')
            words2 = [w.strip().lower() for w in text2.split() if len(w.strip()) >= 2]
            all_texts.extend(words2)
        except:
            pass
        
        # Strategy 3: Inverted (for white text on dark)
        try:
            gray = cv2.cvtColor(rgb_frame, cv2.COLOR_RGB2GRAY) if len(rgb_frame.shape) == 3 else rgb_frame
            inverted = cv2.bitwise_not(gray)
            text3 = pytesseract.image_to_string(inverted, config='--psm 6')
            words3 = [w.strip().lower() for w in text3.split() if len(w.strip()) >= 2]
            all_texts.extend(words3)
        except:
            pass
        
        # Strategy 4: Adaptive threshold
        try:
            gray = cv2.cvtColor(rgb_frame, cv2.COLOR_RGB2GRAY) if len(rgb_frame.shape) == 3 else rgb_frame
            adaptive = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2)
            text4 = pytesseract.image_to_string(adaptive, config='--psm 6')
            words4 = [w.strip().lower() for w in text4.split() if len(w.strip()) >= 2]
            all_texts.extend(words4)
        except:
            pass
        
        # Strategy 5: Try different PSM modes for single words (watermarks are usually short)
        try:
            gray = cv2.cvtColor(rgb_frame, cv2.COLOR_RGB2GRAY) if len(rgb_frame.shape) == 3 else rgb_frame
            enhanced = cv2.convertScaleAbs(gray, alpha=2.5, beta=50)
            # PSM 8 = single word (good for "Sora", "Veo", etc.)
            text5 = pytesseract.image_to_string(enhanced, config='--psm 8')
            words5 = [w.strip().lower() for w in text5.split() if len(w.strip()) >= 2]
            all_texts.extend(words5)
        except:
            pass
        
        # Remove duplicates
        unique_texts = list(set(all_texts))
        
        # Check for watermark keywords in detected text
        for text in unique_texts:
            for keyword in WATERMARK_KEYWORDS:
                keyword_lower = keyword.lower()
                # Check if text contains keyword or keyword contains text (for partial matches)
                if keyword_lower in text or text in keyword_lower:
                    # Calculate confidence based on match quality
                    if text == keyword_lower:
                        confidence = 1.0  # Exact match
                    elif keyword_lower.startswith(text) or text.startswith(keyword_lower):
                        confidence = 0.9  # Very close match
                    else:
                        confidence = 0.7  # Partial match
                    
                    detections.append({
                        'text': text,
                        'keyword': keyword,
                        'confidence': confidence
                    })
                    break  # Only match once per keyword
        
    except Exception as e:
        pass
    
    return detections


def detect_watermarks_in_video(
    video_path: str,
    sample_interval: float = 1.0,  # Sample every N seconds (legacy param, not used)
    min_frames: int = 5,
    max_frames: int = 20,  # Changed to 20 as per ChatGPT approach
    save_watermark_image: bool = False,
) -> Dict:
    """
    Detect watermarks in a video using simple text-based approach.
    
    Algorithm:
    1. Sample frames evenly across video (max 20 frames)
    2. Run OCR on each frame to detect text
    3. Look for watermark keywords (Sora, Veo, Runway, etc.)
    4. If >= 30% of frames contain watermark text, mark as watermarked
    
    Args:
        video_path: Path to video file
        sample_interval: Legacy parameter (not used, frames are sampled evenly)
        min_frames: Minimum frames to sample
        max_frames: Maximum frames to sample (default 20)
    
    Returns:
        Dictionary with watermark detection results
    """
    if not os.path.exists(video_path):
        return {
            'error': f'Video file not found: {video_path}',
            'watermark_detected': False,
            'watermark_confidence': 0.0,
        }
    
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        return {
            'error': f'Could not open video: {video_path}',
            'watermark_detected': False,
            'watermark_confidence': 0.0,
        }
    
    fps = cap.get(cv2.CAP_PROP_FPS)
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    duration = total_frames / fps if fps > 0 else 0
    
    if total_frames == 0:
        cap.release()
        return {
            'error': 'Video has 0 frames',
            'watermark_detected': False,
            'watermark_confidence': 0.0,
        }
    
    # Sample frames evenly (ChatGPT approach)
    frame_indices = sample_frame_indices(total_frames, max_samples=max_frames)
    num_samples = len(frame_indices)
    
    print(f"[INFO] Video: {duration:.2f}s, {total_frames} frames @ {fps:.2f} fps")
    print(f"[INFO] Sampling {num_samples} frames evenly across video")
    
    # Extract sampled frames
    sampled_frames = []
    for frame_idx in frame_indices:
        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
        ret, frame = cap.read()
        if ret:
            sampled_frames.append((frame_idx, frame))
    
    cap.release()
    
    if not sampled_frames:
        return {
            'error': 'No frames could be extracted',
            'watermark_detected': False,
            'watermark_confidence': 0.0,
        }
    
    print(f"[INFO] Extracted {len(sampled_frames)} frames")
    
    # Initialize results
    results = {
        'watermark_detected': False,
        'watermark_confidence': 0.0,
        'watermark_type': 'unknown',
        'watermark_location': [],
        'watermark_text': [],
        'detection_details': {},
    }
    
    # Process each frame: look for watermark text
    frames_with_watermark = 0
    all_detections = []
    detected_keywords = set()
    
    for frame_idx, frame in sampled_frames:
        detections = extract_watermark_text_from_frame(frame)
        
        if detections:
            frames_with_watermark += 1
            all_detections.append({
                'frame_idx': frame_idx,
                'detections': detections
            })
            # Collect unique keywords found
            for det in detections:
                detected_keywords.add(det['keyword'])
    
    # Calculate watermark score
    frames_checked = len(sampled_frames)
    ratio = frames_with_watermark / frames_checked if frames_checked > 0 else 0.0
    
    # Threshold: >= 30% of frames must have watermark text
    WATERMARK_THRESHOLD = 0.3
    has_watermark = ratio >= WATERMARK_THRESHOLD
    
    if has_watermark:
        results['watermark_detected'] = True
        results['watermark_confidence'] = min(ratio, 1.0)  # Confidence = ratio
        results['watermark_type'] = 'text'
        results['watermark_text'] = list(detected_keywords)
        results['watermark_location'] = ['overlay']  # Text overlays can be anywhere
        results['detection_details'] = {
            'frames_checked': frames_checked,
            'frames_with_watermark': frames_with_watermark,
            'ratio': ratio,
            'detections': all_detections,
        }
    else:
        results['detection_details'] = {
            'frames_checked': frames_checked,
            'frames_with_watermark': frames_with_watermark,
            'ratio': ratio,
            'threshold': WATERMARK_THRESHOLD,
        }
    
    return results


def main():
    parser = argparse.ArgumentParser(
        description='Detect watermarks in AI-generated videos',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  python detect_watermarks.py video.mp4
  python detect_watermarks.py video.mp4 --interval 0.5 --min-frames 10
  python detect_watermarks.py video.mp4 --output results.json
        """
    )
    parser.add_argument('video', type=str, help='Path to video file')
    parser.add_argument(
        '--interval', type=float, default=1.0,
        help='Seconds between sampled frames (default: 1.0)'
    )
    parser.add_argument(
        '--min-frames', type=int, default=5,
        help='Minimum frames to sample (default: 5)'
    )
    parser.add_argument(
        '--max-frames', type=int, default=30,
        help='Maximum frames to sample (default: 30)'
    )
    parser.add_argument(
        '--output', type=str, default=None,
        help='Output JSON file path (optional)'
    )
    parser.add_argument(
        '--verbose', action='store_true',
        help='Print detailed detection information'
    )
    parser.add_argument(
        '--save-image', action='store_true',
        help='Save extracted watermark image to file'
    )
    
    args = parser.parse_args()
    
    print("=" * 60)
    print("Watermark Detection Script")
    print("=" * 60)
    print(f"Video: {args.video}")
    print(f"Sample interval: {args.interval}s")
    print(f"Frame range: {args.min_frames}-{args.max_frames}")
    print("=" * 60)
    
    results = detect_watermarks_in_video(
        args.video,
        sample_interval=args.interval,
        min_frames=args.min_frames,
        max_frames=args.max_frames,
        save_watermark_image=args.save_image,
    )
    
    # Print results
    print("\n" + "=" * 60)
    print("DETECTION RESULTS")
    print("=" * 60)
    
    if 'error' in results:
        print(f"ERROR: {results['error']}")
        return
    
    print(f"Watermark Detected: {results['watermark_detected']}")
    print(f"Confidence: {results['watermark_confidence']:.2%}")
    print(f"Type: {results['watermark_type']}")
    
    if results['watermark_location']:
        print(f"Locations: {', '.join(results['watermark_location'])}")
    
    if results['watermark_text']:
        print(f"Text Found: {', '.join(set(results['watermark_text']))}")
    
    if 'identified_logo' in results.get('detection_details', {}):
        logo_info = results['detection_details']['identified_logo']
        logo_conf = results['detection_details'].get('logo_confidence', 0.0)
        print(f"Identified Logo: {logo_info} (confidence: {logo_conf:.2%})")
    
    if args.verbose:
        print("\nDetailed Results:")
        print(json.dumps(results['detection_details'], indent=2, default=str))
    
    # Save to file if requested
    if args.output:
        with open(args.output, 'w') as f:
            json.dump(results, f, indent=2, default=str)
        print(f"\nResults saved to: {args.output}")
    
    print("=" * 60)


if __name__ == '__main__':
    main()

