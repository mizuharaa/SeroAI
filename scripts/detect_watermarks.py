#!/usr/bin/env python3
"""
Efficient watermark detection script for AI-generated videos.
Detects watermarks from SoraAI, Veo, Runway, and other AI video generators.

Strategy:
- Sample frames at intervals (not every frame)
- Focus on common watermark locations (corners, bottom center)
- Use OCR for text watermarks
- Use pattern matching for logos/consistent overlays
- Check for persistent elements across frames
"""

import cv2
import numpy as np
import argparse
import os
from typing import Dict, List, Tuple, Optional
from pathlib import Path
import json

# Optional dependencies
try:
    import pytesseract
    HAS_TESSERACT = True
except ImportError:
    HAS_TESSERACT = False
    print("[WARNING] pytesseract not installed. Text watermark detection will be limited.")

try:
    from Levenshtein import distance as levenshtein_distance
    HAS_LEVENSHTEIN = True
except ImportError:
    HAS_LEVENSHTEIN = False
    print("[WARNING] python-Levenshtein not installed. Text matching will be less accurate.")


# Known watermark keywords and patterns
WATERMARK_KEYWORDS = [
    'sora', 'openai', 'soraai',
    'veo', 'google',
    'runway', 'runwayml',
    'pika', 'pikalabs',
    'kling', 'klingai',
    'stability', 'stable video',
    'meta', 'make-a-video',
    'gen-2', 'gen2',
    'ai generated', 'generated by',
]

# Watermark locations to check (normalized coordinates: [x, y, width, height])
WATERMARK_REGIONS = {
    'top_left': (0.0, 0.0, 0.15, 0.15),
    'top_right': (0.85, 0.0, 0.15, 0.15),
    'bottom_left': (0.0, 0.85, 0.15, 0.15),
    'bottom_right': (0.85, 0.85, 0.15, 0.15),
    'bottom_center': (0.40, 0.90, 0.20, 0.10),
    'center': (0.45, 0.45, 0.10, 0.10),  # Less common but some watermarks appear here
}


def extract_region(frame: np.ndarray, region: Tuple[float, float, float, float]) -> np.ndarray:
    """Extract a region from frame based on normalized coordinates."""
    h, w = frame.shape[:2]
    x, y, rw, rh = region
    x1, y1 = int(x * w), int(y * h)
    x2, y2 = int((x + rw) * w), int((y + rh) * h)
    return frame[y1:y2, x1:x2]


def detect_text_in_region(region: np.ndarray) -> List[str]:
    """Detect text in a region using OCR."""
    if not HAS_TESSERACT:
        return []
    
    try:
        # Preprocess for better OCR
        gray = cv2.cvtColor(region, cv2.COLOR_BGR2GRAY) if len(region.shape) == 3 else region
        
        # Enhance contrast
        gray = cv2.convertScaleAbs(gray, alpha=1.5, beta=30)
        
        # OCR with config for small text
        text = pytesseract.image_to_string(
            gray,
            config='--psm 7 -c tessedit_char_whitelist=abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789- '
        )
        return [t.strip().lower() for t in text.split() if len(t.strip()) > 2]
    except Exception as e:
        return []


def check_keyword_match(texts: List[str], keywords: List[str]) -> Tuple[bool, str, float]:
    """Check if any detected text matches watermark keywords."""
    if not texts:
        return False, "", 0.0
    
    best_match = ""
    best_confidence = 0.0
    
    for text in texts:
        for keyword in keywords:
            if HAS_LEVENSHTEIN:
                # Fuzzy matching
                dist = levenshtein_distance(text.lower(), keyword.lower())
                max_len = max(len(text), len(keyword))
                similarity = 1.0 - (dist / max_len) if max_len > 0 else 0.0
                
                if similarity > 0.7:  # 70% similarity threshold
                    if similarity > best_confidence:
                        best_confidence = similarity
                        best_match = keyword
            else:
                # Exact substring match
                if keyword.lower() in text.lower() or text.lower() in keyword.lower():
                    best_confidence = 1.0
                    best_match = keyword
                    break
    
    return best_confidence > 0.0, best_match, best_confidence


def detect_persistent_pattern(frames: List[np.ndarray], region: Tuple[float, float, float, float]) -> float:
    """Detect if a pattern persists across frames (indicates watermark)."""
    if len(frames) < 3:
        return 0.0
    
    regions = [extract_region(frame, region) for frame in frames]
    
    # Convert to grayscale and normalize
    gray_regions = []
    for r in regions:
        if len(r.shape) == 3:
            gray = cv2.cvtColor(r, cv2.COLOR_BGR2GRAY)
        else:
            gray = r
        gray_regions.append(cv2.resize(gray, (64, 64)))  # Normalize size
    
    # Calculate structural similarity between consecutive frames
    similarities = []
    for i in range(len(gray_regions) - 1):
        # Use template matching or histogram comparison
        hist1 = cv2.calcHist([gray_regions[i]], [0], None, [256], [0, 256])
        hist2 = cv2.calcHist([gray_regions[i+1]], [0], None, [256], [0, 256])
        similarity = cv2.compareHist(hist1, hist2, cv2.HISTCMP_CORREL)
        similarities.append(similarity)
    
    # High similarity across frames = persistent watermark
    avg_similarity = float(np.mean(similarities)) if similarities else 0.0
    return avg_similarity


def detect_logo_pattern(region: np.ndarray) -> float:
    """Detect logo-like patterns (high contrast, distinct shapes)."""
    if len(region.shape) == 3:
        gray = cv2.cvtColor(region, cv2.COLOR_BGR2GRAY)
    else:
        gray = region
    
    # Check for high contrast (typical of logos)
    edges = cv2.Canny(gray, 50, 150)
    edge_density = np.sum(edges > 0) / (edges.shape[0] * edges.shape[1])
    
    # Check for distinct color regions (logos often have solid colors)
    if len(region.shape) == 3:
        region_array = np.asarray(region)
        std_dev = float(np.std(region_array, axis=2).mean())  # type: ignore[arg-type]
    else:
        gray_array = np.asarray(gray)
        std_dev = float(np.std(gray_array))  # type: ignore[arg-type]
    
    # Combine metrics
    logo_score = (edge_density * 0.5) + (min(std_dev / 50.0, 1.0) * 0.5)
    return logo_score


def extract_watermark_image(frames: List[np.ndarray], region: Tuple[float, float, float, float], output_path: Optional[str] = None) -> np.ndarray:
    """Extract and average watermark region across frames to get a clear image."""
    regions = [extract_region(frame, region) for frame in frames]
    
    # Average the regions to get a clearer watermark image
    if regions:
        # Resize all to same size
        target_size = (regions[0].shape[1], regions[0].shape[0])
        resized = [cv2.resize(r, target_size) for r in regions]
        
        # Average - convert to numpy array first
        resized_array = np.asarray(resized)
        avg_region = np.mean(resized_array, axis=0).astype(np.uint8)  # type: ignore[arg-type]
        
        # Enhance contrast
        if len(avg_region.shape) == 3:
            gray = cv2.cvtColor(avg_region, cv2.COLOR_BGR2GRAY)
        else:
            gray = avg_region
        
        # Apply threshold to make watermark more visible
        _, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
        
        if output_path:
            cv2.imwrite(output_path, thresh)
            print(f"[INFO] Watermark image saved to: {output_path}")
        
        return thresh
    return np.array([])


def identify_logo_from_text(region: np.ndarray) -> Tuple[str, float]:
    """Try to identify logo by reading text in the region."""
    if not HAS_TESSERACT:
        return "", 0.0
    
    texts = detect_text_in_region(region)
    has_match, matched_keyword, confidence = check_keyword_match(texts, WATERMARK_KEYWORDS)
    
    if has_match:
        return matched_keyword, confidence
    
    # Also try to read any text found
    if texts:
        return texts[0], 0.5  # Lower confidence for unmatched text
    
    return "", 0.0


def detect_watermarks_in_video(
    video_path: str,
    sample_interval: float = 1.0,  # Sample every N seconds
    min_frames: int = 5,
    max_frames: int = 30,
    save_watermark_image: bool = False,
) -> Dict:
    """
    Efficiently detect watermarks in a video.
    
    Args:
        video_path: Path to video file
        sample_interval: Seconds between sampled frames
        min_frames: Minimum frames to sample
        max_frames: Maximum frames to sample
    
    Returns:
        Dictionary with watermark detection results
    """
    if not os.path.exists(video_path):
        return {
            'error': f'Video file not found: {video_path}',
            'watermark_detected': False,
        }
    
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        return {
            'error': f'Could not open video: {video_path}',
            'watermark_detected': False,
        }
    
    fps = cap.get(cv2.CAP_PROP_FPS)
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    duration = total_frames / fps if fps > 0 else 0
    
    # Calculate sampling strategy
    frame_interval = max(1, int(fps * sample_interval))
    num_samples = min(max_frames, max(min_frames, total_frames // frame_interval))
    
    print(f"[INFO] Video: {duration:.2f}s, {total_frames} frames @ {fps:.2f} fps")
    print(f"[INFO] Sampling {num_samples} frames (every {frame_interval} frames)")
    
    # Sample frames
    sampled_frames = []
    frame_indices = []
    frame_idx = 0
    
    while len(sampled_frames) < num_samples and frame_idx < total_frames:
        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
        ret, frame = cap.read()
        if ret:
            sampled_frames.append(frame)
            frame_indices.append(frame_idx)
        frame_idx += frame_interval
    
    cap.release()
    
    if not sampled_frames:
        return {
            'error': 'No frames could be extracted',
            'watermark_detected': False,
        }
    
    print(f"[INFO] Extracted {len(sampled_frames)} frames")
    
    # Analyze each watermark region
    results = {
        'watermark_detected': False,
        'watermark_confidence': 0.0,
        'watermark_type': 'unknown',
        'watermark_location': [],
        'watermark_text': [],
        'detection_details': {},
    }
    
    region_scores = {}
    text_matches = {}
    persistent_patterns = {}
    
    # Check each region across all sampled frames
    for region_name, region_coords in WATERMARK_REGIONS.items():
        print(f"[INFO] Checking region: {region_name}")
        
        # Extract regions from all frames
        regions = [extract_region(frame, region_coords) for frame in sampled_frames]
        
        # 1. Text detection (OCR)
        all_texts = []
        for region in regions:
            texts = detect_text_in_region(region)
            all_texts.extend(texts)
        
        # Check for keyword matches
        has_match, matched_keyword, confidence = check_keyword_match(all_texts, WATERMARK_KEYWORDS)
        if has_match:
            text_matches[region_name] = {
                'keyword': matched_keyword,
                'confidence': confidence,
                'texts_found': list(set(all_texts)),
            }
            results['watermark_detected'] = True
            results['watermark_confidence'] = max(results['watermark_confidence'], confidence)
            results['watermark_text'].append(matched_keyword)
            results['watermark_location'].append(region_name)
        
        # 2. Persistent pattern detection
        persistence_score = detect_persistent_pattern(sampled_frames, region_coords)
        persistent_patterns[region_name] = persistence_score
        
        # 3. Logo pattern detection
        logo_scores = [detect_logo_pattern(region) for region in regions]
        avg_logo_score = np.mean(logo_scores) if logo_scores else 0.0
        
        # Combine scores for this region
        region_score = (
            (confidence * 0.5) +  # Text match confidence
            (persistence_score * 0.3) +  # Persistence across frames
            (avg_logo_score * 0.2)  # Logo-like pattern
        )
        region_scores[region_name] = region_score
    
    # Aggregate results
    if text_matches:
        results['watermark_type'] = 'text'
        results['detection_details']['text_matches'] = text_matches
    
    # Check for high persistence (indicates watermark even without text)
    high_persistence_regions = {
        k: v for k, v in persistent_patterns.items() 
        if v > 0.85  # 85% similarity across frames
    }
    
    identified_logo = ""
    logo_confidence = 0.0
    
    if high_persistence_regions:
        # Try to identify the logo
        best_region_name = max(high_persistence_regions.items(), key=lambda x: x[1])[0]
        best_region_coords = WATERMARK_REGIONS[best_region_name]
        
        # Extract watermark image from best region
        watermark_image = extract_watermark_image(
            sampled_frames, 
            best_region_coords,
            output_path=f"watermark_{best_region_name}.png" if save_watermark_image else None
        )
        
        # Try to identify logo from text
        if watermark_image.size > 0:
            # Convert back to BGR for OCR if needed
            if len(watermark_image.shape) == 2:
                watermark_bgr = cv2.cvtColor(watermark_image, cv2.COLOR_GRAY2BGR)
            else:
                watermark_bgr = watermark_image
            
            identified_logo, logo_confidence = identify_logo_from_text(watermark_bgr)
        
        if not results['watermark_detected']:
            # Likely a logo/visual watermark
            results['watermark_detected'] = True
            results['watermark_confidence'] = high_persistence_regions[best_region_name] * 0.7  # Lower confidence for non-text
            results['watermark_type'] = 'logo'
            results['watermark_location'] = [best_region_name]
            results['detection_details']['persistent_regions'] = high_persistence_regions
        
        # Add logo identification if found
        if identified_logo:
            results['watermark_text'].append(identified_logo)
            results['detection_details']['identified_logo'] = identified_logo
            results['detection_details']['logo_confidence'] = logo_confidence
            results['watermark_type'] = 'text_logo' if logo_confidence > 0.7 else 'logo'
    
    # Final confidence calculation
    if results['watermark_detected']:
        # Boost confidence if multiple regions show evidence
        evidence_count = len(results['watermark_location'])
        if evidence_count > 1:
            results['watermark_confidence'] = min(1.0, results['watermark_confidence'] * 1.2)
    
    results['detection_details']['region_scores'] = region_scores
    results['detection_details']['persistent_patterns'] = persistent_patterns
    results['detection_details']['frames_analyzed'] = len(sampled_frames)
    results['detection_details']['sample_interval'] = sample_interval
    
    return results


def main():
    parser = argparse.ArgumentParser(
        description='Detect watermarks in AI-generated videos',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  python detect_watermarks.py video.mp4
  python detect_watermarks.py video.mp4 --interval 0.5 --min-frames 10
  python detect_watermarks.py video.mp4 --output results.json
        """
    )
    parser.add_argument('video', type=str, help='Path to video file')
    parser.add_argument(
        '--interval', type=float, default=1.0,
        help='Seconds between sampled frames (default: 1.0)'
    )
    parser.add_argument(
        '--min-frames', type=int, default=5,
        help='Minimum frames to sample (default: 5)'
    )
    parser.add_argument(
        '--max-frames', type=int, default=30,
        help='Maximum frames to sample (default: 30)'
    )
    parser.add_argument(
        '--output', type=str, default=None,
        help='Output JSON file path (optional)'
    )
    parser.add_argument(
        '--verbose', action='store_true',
        help='Print detailed detection information'
    )
    parser.add_argument(
        '--save-image', action='store_true',
        help='Save extracted watermark image to file'
    )
    
    args = parser.parse_args()
    
    print("=" * 60)
    print("Watermark Detection Script")
    print("=" * 60)
    print(f"Video: {args.video}")
    print(f"Sample interval: {args.interval}s")
    print(f"Frame range: {args.min_frames}-{args.max_frames}")
    print("=" * 60)
    
    results = detect_watermarks_in_video(
        args.video,
        sample_interval=args.interval,
        min_frames=args.min_frames,
        max_frames=args.max_frames,
        save_watermark_image=args.save_image,
    )
    
    # Print results
    print("\n" + "=" * 60)
    print("DETECTION RESULTS")
    print("=" * 60)
    
    if 'error' in results:
        print(f"ERROR: {results['error']}")
        return
    
    print(f"Watermark Detected: {results['watermark_detected']}")
    print(f"Confidence: {results['watermark_confidence']:.2%}")
    print(f"Type: {results['watermark_type']}")
    
    if results['watermark_location']:
        print(f"Locations: {', '.join(results['watermark_location'])}")
    
    if results['watermark_text']:
        print(f"Text Found: {', '.join(set(results['watermark_text']))}")
    
    if 'identified_logo' in results.get('detection_details', {}):
        logo_info = results['detection_details']['identified_logo']
        logo_conf = results['detection_details'].get('logo_confidence', 0.0)
        print(f"Identified Logo: {logo_info} (confidence: {logo_conf:.2%})")
    
    if args.verbose:
        print("\nDetailed Results:")
        print(json.dumps(results['detection_details'], indent=2, default=str))
    
    # Save to file if requested
    if args.output:
        with open(args.output, 'w') as f:
            json.dump(results, f, indent=2, default=str)
        print(f"\nResults saved to: {args.output}")
    
    print("=" * 60)


if __name__ == '__main__':
    main()

